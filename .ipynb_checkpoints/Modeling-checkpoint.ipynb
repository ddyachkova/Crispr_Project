{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darya\\AppData\\Roaming\\Python\\Python36\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, explained_variance_score, r2_score, roc_auc_score, precision_score, f1_score\n",
    "\n",
    "from modules import *\n",
    "from models import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i    2989\n",
       "j       2\n",
       "x       1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSC_data.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSC_data = pd.read_csv(\"CSC_repat.csv\")\n",
    "y_train = pd.read_csv(\"screen_stats_adj.csv\")\n",
    "y_train = y_train.replace(-1, 2)\n",
    "\n",
    "\n",
    "class_weighs = ((len(y_train)/y_train.iloc[:, 1].value_counts()).values)\n",
    "\n",
    "CSC_data = CSC_data.iloc[:, 1:]\n",
    "# y_train = y_train.iloc[:, 1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X_path, y_path, dset_len, chunksize):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.chunksize = chunksize\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.dset_len = dset_len\n",
    "        self.reader_X = pd.read_csv(self.X_path) #, chunksize = self.chunksize) #, iterator=True)\n",
    "        self.reader_y = pd.read_csv(self.y_path, chunksize = self.chunksize) #, iterator=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.X = self.get_batch(self.reader_X, self.chunksize, idx)\n",
    "        self.y = self.reader_y.get_chunk(self.chunksize)\n",
    "        self.y = self.y.iloc[:, 1:].replace(-1, 2).values\n",
    "        \n",
    "        return torch.tensor(self.X), torch.tensor(self.y)\n",
    "    \n",
    "    def get_batch(self, reader_X, chunksize, idx):\n",
    "        X_batch = reader_X[(reader_X.iloc[:, 0] >= chunksize*idx) & (reader_X.iloc[:, 0] < chunksize*(idx + 1))]\n",
    "        indices = X_batch.iloc[:, 0].values - chunksize*idx\n",
    "        indptr = X_batch.iloc[:, 1].values\n",
    "        data = X_batch.iloc[:, 2].values\n",
    "        mtx = sparse.csc_matrix((data, (indices, indptr)), shape=(chunksize, 149321)).toarray()\n",
    "        return mtx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "chunksize = 32\n",
    "for idx in [0, 1, 2, 3]:\n",
    "    X_batch = reader_X[(reader_X.iloc[:, 0] >= chunksize*idx) & (reader_X.iloc[:, 0] < chunksize*(idx + 1))]\n",
    "    indices = X_batch.iloc[:, 0].values - chunksize*idx\n",
    "    indptr = X_batch.iloc[:, 1].values\n",
    "    data = X_batch.iloc[:, 2].values\n",
    "    \n",
    "    mtx = sparse.csc_matrix((data, (indices, indptr)), shape=(chunksize, 149321)).toarray()\n",
    "\n",
    "    print (mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(83539/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83539, 5)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i     81304\n",
       "j    149320\n",
       "x         1\n",
       "Name: 2906911, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSC_data.iloc[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_iter = iter(custom_dset)\n",
    "# for i in range(5):\n",
    "#     print (next(dset_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dset = Dset(\"CSC_repat.csv\", \"screen_stats_adj.csv\", 83539, batch_size)\n",
    "\n",
    "dataset_indices = list(range(len(custom_dset)))\n",
    "np.random.shuffle(dataset_indices)\n",
    "val_split_index = int(np.floor(0.2 * len(dataset_indices)))\n",
    "\n",
    "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=custom_dset, batch_size=1, shuffle=False, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset=custom_dset, batch_size=1, shuffle=False, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(y_train, CSC_data, batch_size, \n",
    "#       n_iter, model, optimizer, \n",
    "#       criterion, n_epochs, scheduler, \n",
    "#       res_name=None, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardBin_notebook(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, batch_size):\n",
    "        super(FeedforwardBin_notebook, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1  = hidden_size1\n",
    "        self.hidden_size2  = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size1)\n",
    "        self.tahn1 = torch.nn.Tanh()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.tahn2 = torch.nn.Tanh()            \n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size2, self.hidden_size3)\n",
    "        self.tahn3 = torch.nn.Tanh()            \n",
    "#         self.fc4 = torch.nn.Linear(self.hidden_size3, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(self.hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden1 = self.fc1(x)\n",
    "        hidden1 = self.batchnorm1(hidden1)\n",
    "        tahn1 = self.tahn1(hidden1)\n",
    "        tahn1 = self.dropout1(tahn1)\n",
    "        hidden2 = self.fc2(tahn1)     \n",
    "        tahn2 = self.tahn2(hidden2)\n",
    "        tahn2 = self.dropout2(tahn2)        \n",
    "        fc3 = self.fc3(tahn2)\n",
    "        fc3 = fc3.view(self.batch_size, 4, 3)\n",
    "\n",
    "        \n",
    "#         tahn3 = self.tahn3(fc3)\n",
    "#         output = self.fc4(tahn3)\n",
    "\n",
    "#         output = self.softmax(fc3)\n",
    "        return fc3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "dim1, dim2, dim3, dim4 = 149321, 512, 128, 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardBin_notebook(dim1, dim2, dim3, dim4, batch_size)\n",
    "class_weights = [1.5, 5.6, 6.4]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = torch.tensor(class_weights).cuda())\n",
    "\n",
    "# res_name = result_name(args.res_dir, args.num_epochs, args.model_class, dim1, dim2, dim3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(model, criterion, epoch):\n",
    "    loss_ = []\n",
    "    a = time()\n",
    "    model.eval()\n",
    "    monitor_step = 10\n",
    "    for i, data in enumerate(val_loader):\n",
    "        X, y = data\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        X = X.reshape(X.shape[1], X.shape[-1]).float()\n",
    "        y = y.reshape(y.shape[1], y.shape[-1]).long()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "#         loss = criterion(y_pred, y)\n",
    "        loss = 0\n",
    "        for i in range(0, 4):\n",
    "            loss__ = criterion(y_pred[:, i, :], y[:, i])\n",
    "            loss += loss__\n",
    "        loss_ = np.append(loss_, loss.item())\n",
    "#         pres, f1 = calc_pres(y, y_pred)\n",
    "        loss_ = np.array(loss_)  \n",
    "\n",
    "        if i % monitor_step == 0: \n",
    "            print('Iteration {}: val loss: {} precision {}, f-1 {}'.format(i, loss.item(), pres, f1))\n",
    "    s = '%d: Val loss:%f, MSE: N samples: %d in %f min'%(epoch, loss_.mean(), len(loss_), (time() -a)/60.)\n",
    "    print(s)\n",
    "    return loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs, scheduler, debug=False):\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    a = time()\n",
    "    epoch = 0\n",
    "    monitor_step = 2\n",
    "    val_loss_, train_loss_, pres_ = [], [], []\n",
    "    for e in range(n_epochs):\n",
    "        epoch = e+1\n",
    "        print('>>>>>>>Epoch %d'%(epoch))\n",
    "        print(\">>>>>>> Training\")\n",
    "        for i, data in enumerate(train_loader):\n",
    "            X_tr, y_tr = data\n",
    "            X_tr = X_tr.reshape(X_tr.shape[1], X_tr.shape[-1]).float().cuda()\n",
    "            y_tr = y_tr.reshape(y_tr.shape[1], y_tr.shape[-1]).long().cuda()\n",
    "\n",
    "\n",
    "            y_pred = model(X_tr)\n",
    "\n",
    "            if debug:\n",
    "                print ('X', X_tr.shape)\n",
    "                print ('y_tr', y_tr.shape)\n",
    "                print ('y_pred', y_pred.shape)\n",
    "            \n",
    "            loss = 0\n",
    "            for j in range(0, 4):\n",
    "                loss_ = criterion(y_pred[:, j, :], y_tr[:, j])\n",
    "                loss += loss_\n",
    "                \n",
    "#             pres, f1 = calc_pres(y_tr, y_pred)\n",
    "\n",
    "\n",
    "            train_loss_.append(loss.item())\n",
    "#             pres_.append(pres)\n",
    "            if i % monitor_step == 0: \n",
    "#                 print('Epoch {} iteration {}: train loss: {} precision {} f1 {}'.format(epoch, i, loss.item(), pres, f1))\n",
    "                print('Epoch {} iteration {}: train loss: {}'.format(epoch, i, loss.item()))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = do_eval(CSC_data, y_train, num_iter, model, criterion, epoch)\n",
    "        val_loss_.append(val_loss)\n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "\n",
    "    print('%d: Train time:%.2f min in %d steps'%(epoch, (time() - a)/60, n_iter))\n",
    "#     model_save(model, optimizer, n_epochs, train_loss_, val_loss_, res_name)\n",
    "    return train_loss_, val_loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>Epoch 1\n",
      ">>>>>>> Training\n",
      "Epoch 1 iteration 0: train loss: 5.406485080718994\n",
      "Epoch 1 iteration 2: train loss: 6.098917007446289\n",
      "Epoch 1 iteration 4: train loss: 5.307425498962402\n",
      "Epoch 1 iteration 6: train loss: 5.1923418045043945\n",
      "Epoch 1 iteration 8: train loss: 5.168911933898926\n",
      "Epoch 1 iteration 10: train loss: 5.526158332824707\n",
      "Epoch 1 iteration 12: train loss: 4.879940986633301\n",
      "Epoch 1 iteration 14: train loss: 5.227667808532715\n",
      "Epoch 1 iteration 16: train loss: 5.245072841644287\n",
      "Epoch 1 iteration 18: train loss: 5.16267204284668\n",
      "Epoch 1 iteration 20: train loss: 6.001282691955566\n",
      "Epoch 1 iteration 22: train loss: 5.711440563201904\n",
      "Epoch 1 iteration 24: train loss: 5.6781840324401855\n",
      "Epoch 1 iteration 26: train loss: 4.730396270751953\n",
      "Epoch 1 iteration 28: train loss: 5.001768112182617\n",
      "Epoch 1 iteration 30: train loss: 5.252071857452393\n",
      "Epoch 1 iteration 32: train loss: 5.439499378204346\n",
      "Epoch 1 iteration 34: train loss: 5.718565464019775\n",
      "Epoch 1 iteration 36: train loss: 4.801582336425781\n",
      "Epoch 1 iteration 38: train loss: 4.781128883361816\n",
      "Epoch 1 iteration 40: train loss: 4.968202114105225\n",
      "Epoch 1 iteration 42: train loss: 5.046992778778076\n",
      "Epoch 1 iteration 44: train loss: 4.503298759460449\n",
      "Epoch 1 iteration 46: train loss: 5.1853837966918945\n",
      "Epoch 1 iteration 48: train loss: 5.561738014221191\n",
      "Epoch 1 iteration 50: train loss: 4.810646057128906\n",
      "Epoch 1 iteration 52: train loss: 4.532863140106201\n",
      "Epoch 1 iteration 54: train loss: 3.5608179569244385\n",
      "Epoch 1 iteration 56: train loss: 3.9099106788635254\n",
      "Epoch 1 iteration 58: train loss: 4.878387451171875\n",
      "Epoch 1 iteration 60: train loss: 4.164832592010498\n",
      "Epoch 1 iteration 62: train loss: 3.5411999225616455\n",
      "Epoch 1 iteration 64: train loss: 5.751115322113037\n",
      "Epoch 1 iteration 66: train loss: 5.817171096801758\n",
      "Epoch 1 iteration 68: train loss: 5.6106858253479\n",
      "Epoch 1 iteration 70: train loss: 5.3060455322265625\n",
      "Epoch 1 iteration 72: train loss: 5.978046417236328\n",
      "Epoch 1 iteration 74: train loss: 4.7048420906066895\n",
      "Epoch 1 iteration 76: train loss: 4.534255027770996\n",
      "Epoch 1 iteration 78: train loss: 5.340939998626709\n",
      "Epoch 1 iteration 80: train loss: 5.77601432800293\n",
      "Epoch 1 iteration 82: train loss: 4.0507893562316895\n",
      "Epoch 1 iteration 84: train loss: 4.147541046142578\n",
      "Epoch 1 iteration 86: train loss: 4.67478084564209\n",
      "Epoch 1 iteration 88: train loss: 4.425143241882324\n",
      "Epoch 1 iteration 90: train loss: 4.565618991851807\n",
      "Epoch 1 iteration 92: train loss: 5.350564956665039\n",
      "Epoch 1 iteration 94: train loss: 5.187209606170654\n",
      "Epoch 1 iteration 96: train loss: 4.736808776855469\n",
      "Epoch 1 iteration 98: train loss: 4.0979204177856445\n",
      "Epoch 1 iteration 100: train loss: 5.350838661193848\n",
      "Epoch 1 iteration 102: train loss: 5.7474751472473145\n",
      "Epoch 1 iteration 104: train loss: 4.614596366882324\n",
      "Epoch 1 iteration 106: train loss: 5.226372718811035\n",
      "Epoch 1 iteration 108: train loss: 5.296507358551025\n",
      "Epoch 1 iteration 110: train loss: 5.897097587585449\n",
      "Epoch 1 iteration 112: train loss: 5.291257858276367\n",
      "Epoch 1 iteration 114: train loss: 5.463361740112305\n",
      "Epoch 1 iteration 116: train loss: 5.364348411560059\n",
      "Epoch 1 iteration 118: train loss: 4.546828746795654\n",
      "Epoch 1 iteration 120: train loss: 4.171633720397949\n",
      "Epoch 1 iteration 122: train loss: 5.927590370178223\n",
      "Epoch 1 iteration 124: train loss: 5.054319858551025\n",
      "Epoch 1 iteration 126: train loss: 4.910050868988037\n",
      "Epoch 1 iteration 128: train loss: 4.608311653137207\n",
      "Epoch 1 iteration 130: train loss: 4.200010299682617\n",
      "Epoch 1 iteration 132: train loss: 4.979351997375488\n",
      "Epoch 1 iteration 134: train loss: 5.1035027503967285\n",
      "Epoch 1 iteration 136: train loss: 5.009763240814209\n",
      "Epoch 1 iteration 138: train loss: 4.391850471496582\n",
      "Epoch 1 iteration 140: train loss: 5.132368564605713\n",
      "Epoch 1 iteration 142: train loss: 5.789863586425781\n",
      "Epoch 1 iteration 144: train loss: 5.728520393371582\n",
      "Epoch 1 iteration 146: train loss: 5.51525354385376\n",
      "Epoch 1 iteration 148: train loss: 5.7017822265625\n",
      "Epoch 1 iteration 150: train loss: 5.664421558380127\n",
      "Epoch 1 iteration 152: train loss: 3.97444224357605\n",
      "Epoch 1 iteration 154: train loss: 5.632784366607666\n",
      "Epoch 1 iteration 156: train loss: 5.194270610809326\n",
      "Epoch 1 iteration 158: train loss: 6.825845718383789\n",
      "Epoch 1 iteration 160: train loss: 5.253565788269043\n",
      "Epoch 1 iteration 162: train loss: 5.710593223571777\n",
      "Epoch 1 iteration 164: train loss: 6.34952449798584\n",
      "Epoch 1 iteration 166: train loss: 5.601603031158447\n",
      "Epoch 1 iteration 168: train loss: 5.220427989959717\n",
      "Epoch 1 iteration 170: train loss: 5.602560520172119\n",
      "Epoch 1 iteration 172: train loss: 5.29702091217041\n",
      "Epoch 1 iteration 174: train loss: 5.836464881896973\n",
      "Epoch 1 iteration 176: train loss: 5.757699966430664\n",
      "Epoch 1 iteration 178: train loss: 5.870123863220215\n",
      "Epoch 1 iteration 180: train loss: 5.212308406829834\n",
      "Epoch 1 iteration 182: train loss: 5.101469039916992\n",
      "Epoch 1 iteration 184: train loss: 6.3987345695495605\n",
      "Epoch 1 iteration 186: train loss: 3.9976634979248047\n",
      "Epoch 1 iteration 188: train loss: 5.892107009887695\n",
      "Epoch 1 iteration 190: train loss: 4.53865385055542\n",
      "Epoch 1 iteration 192: train loss: 6.315432548522949\n",
      "Epoch 1 iteration 194: train loss: 5.405257701873779\n",
      "Epoch 1 iteration 196: train loss: 6.603694915771484\n",
      "Epoch 1 iteration 198: train loss: 4.208714485168457\n",
      "Epoch 1 iteration 200: train loss: 4.293633937835693\n",
      "Epoch 1 iteration 202: train loss: 6.058143615722656\n",
      "Epoch 1 iteration 204: train loss: 7.967103958129883\n",
      "Epoch 1 iteration 206: train loss: 8.408651351928711\n",
      "Epoch 1 iteration 208: train loss: 4.583449363708496\n",
      "Epoch 1 iteration 210: train loss: 5.5989089012146\n",
      "Epoch 1 iteration 212: train loss: 6.076687335968018\n",
      "Epoch 1 iteration 214: train loss: 6.305926322937012\n",
      "Epoch 1 iteration 216: train loss: 5.870833873748779\n",
      "Epoch 1 iteration 218: train loss: 8.454278945922852\n",
      "Epoch 1 iteration 220: train loss: 5.601567268371582\n",
      "Epoch 1 iteration 222: train loss: 6.296311378479004\n",
      "Epoch 1 iteration 224: train loss: 7.166540145874023\n",
      "Epoch 1 iteration 226: train loss: 8.462128639221191\n",
      "Epoch 1 iteration 228: train loss: 8.464312553405762\n",
      "Epoch 1 iteration 230: train loss: 6.784499168395996\n",
      "Epoch 1 iteration 232: train loss: 7.949167251586914\n",
      "Epoch 1 iteration 234: train loss: 8.465642929077148\n",
      "Epoch 1 iteration 236: train loss: 6.948615550994873\n",
      "Epoch 1 iteration 238: train loss: 8.247034072875977\n",
      "Epoch 1 iteration 240: train loss: 7.4817094802856445\n",
      "Epoch 1 iteration 242: train loss: 7.544270992279053\n",
      "Epoch 1 iteration 244: train loss: 6.549142837524414\n",
      "Epoch 1 iteration 246: train loss: 7.244374752044678\n",
      "Epoch 1 iteration 248: train loss: 8.295310974121094\n",
      "Epoch 1 iteration 250: train loss: 6.958446979522705\n",
      "Epoch 1 iteration 252: train loss: 7.415249347686768\n",
      "Epoch 1 iteration 254: train loss: 7.613865375518799\n",
      "Epoch 1 iteration 256: train loss: 5.852573394775391\n",
      "Epoch 1 iteration 258: train loss: 6.888874053955078\n",
      "Epoch 1 iteration 260: train loss: 8.33292293548584\n",
      "Epoch 1 iteration 262: train loss: 9.073274612426758\n",
      "Epoch 1 iteration 264: train loss: 8.747271537780762\n",
      "Epoch 1 iteration 266: train loss: 9.155295372009277\n",
      "Epoch 1 iteration 268: train loss: 8.292101860046387\n",
      "Epoch 1 iteration 270: train loss: 7.873709678649902\n",
      "Epoch 1 iteration 272: train loss: 6.3083319664001465\n",
      "Epoch 1 iteration 274: train loss: 6.964641571044922\n",
      "Epoch 1 iteration 276: train loss: 6.234744548797607\n",
      "Epoch 1 iteration 278: train loss: 5.9534912109375\n",
      "Epoch 1 iteration 280: train loss: 7.090127468109131\n",
      "Epoch 1 iteration 282: train loss: 7.036962509155273\n",
      "Epoch 1 iteration 284: train loss: 7.134573936462402\n",
      "Epoch 1 iteration 286: train loss: 6.31179141998291\n",
      "Epoch 1 iteration 288: train loss: 7.577699661254883\n",
      "Epoch 1 iteration 290: train loss: 8.17549991607666\n",
      "Epoch 1 iteration 292: train loss: 6.6414570808410645\n",
      "Epoch 1 iteration 294: train loss: 7.2804155349731445\n",
      "Epoch 1 iteration 296: train loss: 5.988518714904785\n",
      "Epoch 1 iteration 298: train loss: 8.569559097290039\n",
      "Epoch 1 iteration 300: train loss: 10.31641960144043\n",
      "Epoch 1 iteration 302: train loss: 10.46479606628418\n",
      "Epoch 1 iteration 304: train loss: 10.820414543151855\n",
      "Epoch 1 iteration 306: train loss: 12.694011688232422\n",
      "Epoch 1 iteration 308: train loss: 9.244029998779297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iteration 310: train loss: 8.318323135375977\n",
      "Epoch 1 iteration 312: train loss: 7.761798858642578\n",
      "Epoch 1 iteration 314: train loss: 9.023946762084961\n",
      "Epoch 1 iteration 316: train loss: 8.246610641479492\n",
      "Epoch 1 iteration 318: train loss: 9.95287036895752\n",
      "Epoch 1 iteration 320: train loss: 6.381779670715332\n",
      "Epoch 1 iteration 322: train loss: 7.333621025085449\n",
      "Epoch 1 iteration 324: train loss: 6.047383785247803\n",
      "Epoch 1 iteration 326: train loss: 7.936946868896484\n",
      "Epoch 1 iteration 328: train loss: 8.913434028625488\n",
      "Epoch 1 iteration 330: train loss: 7.057770729064941\n",
      "Epoch 1 iteration 332: train loss: 10.589982032775879\n",
      "Epoch 1 iteration 334: train loss: 9.376014709472656\n",
      "Epoch 1 iteration 336: train loss: 10.148595809936523\n",
      "Epoch 1 iteration 338: train loss: 13.055095672607422\n",
      "Epoch 1 iteration 340: train loss: 6.20089864730835\n",
      "Epoch 1 iteration 342: train loss: 8.43897819519043\n",
      "Epoch 1 iteration 344: train loss: 7.461874961853027\n",
      "Epoch 1 iteration 346: train loss: 9.886327743530273\n",
      "Epoch 1 iteration 348: train loss: 7.561315536499023\n",
      "Epoch 1 iteration 350: train loss: 8.977060317993164\n",
      "Epoch 1 iteration 352: train loss: 7.612149238586426\n",
      "Epoch 1 iteration 354: train loss: 8.392650604248047\n",
      "Epoch 1 iteration 356: train loss: 11.129021644592285\n",
      "Epoch 1 iteration 358: train loss: 9.59642505645752\n",
      "Epoch 1 iteration 360: train loss: 12.471508026123047\n",
      "Epoch 1 iteration 362: train loss: 8.555932998657227\n",
      "Epoch 1 iteration 364: train loss: 6.870558261871338\n",
      "Epoch 1 iteration 366: train loss: 11.335674285888672\n",
      "Epoch 1 iteration 368: train loss: 8.231915473937988\n",
      "Epoch 1 iteration 370: train loss: 7.146036624908447\n",
      "Epoch 1 iteration 372: train loss: 6.364304542541504\n",
      "Epoch 1 iteration 374: train loss: 8.77794075012207\n",
      "Epoch 1 iteration 376: train loss: 8.532068252563477\n",
      "Epoch 1 iteration 378: train loss: 6.89115571975708\n",
      "Epoch 1 iteration 380: train loss: 9.535408020019531\n",
      "Epoch 1 iteration 382: train loss: 14.874988555908203\n",
      "Epoch 1 iteration 384: train loss: 12.818581581115723\n",
      "Epoch 1 iteration 386: train loss: 10.545421600341797\n",
      "Epoch 1 iteration 388: train loss: 13.381264686584473\n",
      "Epoch 1 iteration 390: train loss: 10.391963005065918\n",
      "Epoch 1 iteration 392: train loss: 12.233926773071289\n",
      "Epoch 1 iteration 394: train loss: 8.928436279296875\n",
      "Epoch 1 iteration 396: train loss: 9.854256629943848\n",
      "Epoch 1 iteration 398: train loss: 10.19289779663086\n",
      "Epoch 1 iteration 400: train loss: 12.663745880126953\n",
      "Epoch 1 iteration 402: train loss: 6.779876232147217\n",
      "Epoch 1 iteration 404: train loss: 9.312093734741211\n",
      "Epoch 1 iteration 406: train loss: 10.662019729614258\n",
      "Epoch 1 iteration 408: train loss: 8.794901847839355\n",
      "Epoch 1 iteration 410: train loss: 9.547904014587402\n",
      "Epoch 1 iteration 412: train loss: 8.182207107543945\n",
      "Epoch 1 iteration 414: train loss: 11.784564971923828\n",
      "Epoch 1 iteration 416: train loss: 9.844522476196289\n",
      "Epoch 1 iteration 418: train loss: 12.178248405456543\n",
      "Epoch 1 iteration 420: train loss: 11.066657066345215\n",
      "Epoch 1 iteration 422: train loss: 10.259705543518066\n",
      "Epoch 1 iteration 424: train loss: 8.774484634399414\n",
      "Epoch 1 iteration 426: train loss: 10.674826622009277\n",
      "Epoch 1 iteration 428: train loss: 9.228202819824219\n",
      "Epoch 1 iteration 430: train loss: 11.193758964538574\n",
      "Epoch 1 iteration 432: train loss: 11.646932601928711\n",
      "Epoch 1 iteration 434: train loss: 9.335740089416504\n",
      "Epoch 1 iteration 436: train loss: 9.129846572875977\n",
      "Epoch 1 iteration 438: train loss: 10.561391830444336\n",
      "Epoch 1 iteration 440: train loss: 12.995782852172852\n",
      "Epoch 1 iteration 442: train loss: 11.5771484375\n",
      "Epoch 1 iteration 444: train loss: 11.662763595581055\n",
      "Epoch 1 iteration 446: train loss: 9.930072784423828\n",
      "Epoch 1 iteration 448: train loss: 9.398944854736328\n",
      "Epoch 1 iteration 450: train loss: 8.11760425567627\n",
      "Epoch 1 iteration 452: train loss: 10.646159172058105\n",
      "Epoch 1 iteration 454: train loss: 10.79151725769043\n",
      "Epoch 1 iteration 456: train loss: 12.499601364135742\n",
      "Epoch 1 iteration 458: train loss: 12.45448112487793\n",
      "Epoch 1 iteration 460: train loss: 8.678930282592773\n",
      "Epoch 1 iteration 462: train loss: 9.2982816696167\n",
      "Epoch 1 iteration 464: train loss: 10.000619888305664\n",
      "Epoch 1 iteration 466: train loss: 10.335381507873535\n",
      "Epoch 1 iteration 468: train loss: 14.113863945007324\n",
      "Epoch 1 iteration 470: train loss: 12.039656639099121\n",
      "Epoch 1 iteration 472: train loss: 10.003159523010254\n",
      "Epoch 1 iteration 474: train loss: 12.564427375793457\n",
      "Epoch 1 iteration 476: train loss: 7.406075477600098\n",
      "Epoch 1 iteration 478: train loss: 7.18897008895874\n",
      "Epoch 1 iteration 480: train loss: 7.356949806213379\n",
      "Epoch 1 iteration 482: train loss: 7.876860618591309\n",
      "Epoch 1 iteration 484: train loss: 11.806649208068848\n",
      "Epoch 1 iteration 486: train loss: 8.46367073059082\n",
      "Epoch 1 iteration 488: train loss: 7.362731456756592\n",
      "Epoch 1 iteration 490: train loss: 6.187804698944092\n",
      "Epoch 1 iteration 492: train loss: 8.741182327270508\n",
      "Epoch 1 iteration 494: train loss: 11.330163955688477\n",
      "Epoch 1 iteration 496: train loss: 12.02811050415039\n",
      "Epoch 1 iteration 498: train loss: 11.430542945861816\n",
      "Epoch 1 iteration 500: train loss: 9.676497459411621\n",
      "Epoch 1 iteration 502: train loss: 15.24731159210205\n",
      "Epoch 1 iteration 504: train loss: 13.828433990478516\n",
      "Epoch 1 iteration 506: train loss: 14.217071533203125\n",
      "Epoch 1 iteration 508: train loss: 10.543925285339355\n",
      "Epoch 1 iteration 510: train loss: 11.582225799560547\n",
      "Epoch 1 iteration 512: train loss: 7.037010192871094\n",
      "Epoch 1 iteration 514: train loss: 13.62613296508789\n",
      "Epoch 1 iteration 516: train loss: 11.480813980102539\n",
      "Epoch 1 iteration 518: train loss: 15.482406616210938\n",
      "Epoch 1 iteration 520: train loss: 7.078369617462158\n",
      "Epoch 1 iteration 522: train loss: 10.322193145751953\n",
      "Epoch 1 iteration 524: train loss: 19.893959045410156\n",
      "Epoch 1 iteration 526: train loss: 20.733230590820312\n",
      "Epoch 1 iteration 528: train loss: 12.56515121459961\n",
      "Epoch 1 iteration 530: train loss: 19.546934127807617\n",
      "Epoch 1 iteration 532: train loss: 16.060588836669922\n",
      "Epoch 1 iteration 534: train loss: 12.172492980957031\n",
      "Epoch 1 iteration 536: train loss: 11.38798713684082\n",
      "Epoch 1 iteration 538: train loss: 13.332826614379883\n",
      "Epoch 1 iteration 540: train loss: 14.249519348144531\n",
      "Epoch 1 iteration 542: train loss: 11.625982284545898\n",
      "Epoch 1 iteration 544: train loss: 10.881901741027832\n",
      "Epoch 1 iteration 546: train loss: 9.200382232666016\n",
      "Epoch 1 iteration 548: train loss: 10.281664848327637\n",
      "Epoch 1 iteration 550: train loss: 8.368650436401367\n",
      "Epoch 1 iteration 552: train loss: 14.976040840148926\n",
      "Epoch 1 iteration 554: train loss: 13.453839302062988\n",
      "Epoch 1 iteration 556: train loss: 17.274412155151367\n",
      "Epoch 1 iteration 558: train loss: 15.45473861694336\n",
      "Epoch 1 iteration 560: train loss: 10.916777610778809\n",
      "Epoch 1 iteration 562: train loss: 14.525136947631836\n",
      "Epoch 1 iteration 564: train loss: 17.40096664428711\n",
      "Epoch 1 iteration 566: train loss: 19.217288970947266\n",
      "Epoch 1 iteration 568: train loss: 15.674287796020508\n",
      "Epoch 1 iteration 570: train loss: 11.358707427978516\n",
      "Epoch 1 iteration 572: train loss: 9.520381927490234\n",
      "Epoch 1 iteration 574: train loss: 10.675798416137695\n",
      "Epoch 1 iteration 576: train loss: 9.908523559570312\n",
      "Epoch 1 iteration 578: train loss: 10.731968879699707\n",
      "Epoch 1 iteration 580: train loss: 9.571287155151367\n",
      "Epoch 1 iteration 582: train loss: 12.805248260498047\n",
      "Epoch 1 iteration 584: train loss: 6.744926452636719\n",
      "Epoch 1 iteration 586: train loss: 7.56438684463501\n",
      "Epoch 1 iteration 588: train loss: 18.22571563720703\n",
      "Epoch 1 iteration 590: train loss: 15.73215103149414\n",
      "Epoch 1 iteration 592: train loss: 13.638126373291016\n",
      "Epoch 1 iteration 594: train loss: 13.303316116333008\n",
      "Epoch 1 iteration 596: train loss: 15.468810081481934\n",
      "Epoch 1 iteration 598: train loss: 22.353984832763672\n",
      "Epoch 1 iteration 600: train loss: 18.041723251342773\n",
      "Epoch 1 iteration 602: train loss: 16.037269592285156\n",
      "Epoch 1 iteration 604: train loss: 14.227455139160156\n",
      "Epoch 1 iteration 606: train loss: 12.186701774597168\n",
      "Epoch 1 iteration 608: train loss: 7.794408798217773\n",
      "Epoch 1 iteration 610: train loss: 16.609207153320312\n",
      "Epoch 1 iteration 612: train loss: 11.895101547241211\n",
      "Epoch 1 iteration 614: train loss: 11.57271957397461\n",
      "Epoch 1 iteration 616: train loss: 12.912710189819336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iteration 618: train loss: 14.177087783813477\n",
      "Epoch 1 iteration 620: train loss: 18.215757369995117\n",
      "Epoch 1 iteration 622: train loss: 21.201257705688477\n",
      "Epoch 1 iteration 624: train loss: 11.310361862182617\n",
      "Epoch 1 iteration 626: train loss: 13.347959518432617\n",
      "Epoch 1 iteration 628: train loss: 11.902505874633789\n",
      "Epoch 1 iteration 630: train loss: 17.20671844482422\n",
      "Epoch 1 iteration 632: train loss: 10.49075984954834\n",
      "Epoch 1 iteration 634: train loss: 12.476180076599121\n",
      "Epoch 1 iteration 636: train loss: 12.671510696411133\n",
      "Epoch 1 iteration 638: train loss: 17.634056091308594\n",
      "Epoch 1 iteration 640: train loss: 14.27334213256836\n",
      "Epoch 1 iteration 642: train loss: 11.272771835327148\n",
      "Epoch 1 iteration 644: train loss: 13.081127166748047\n",
      "Epoch 1 iteration 646: train loss: 7.161596775054932\n",
      "Epoch 1 iteration 648: train loss: 16.642690658569336\n",
      "Epoch 1 iteration 650: train loss: 19.779327392578125\n",
      "Epoch 1 iteration 652: train loss: 21.174699783325195\n",
      "Epoch 1 iteration 654: train loss: 17.493465423583984\n",
      "Epoch 1 iteration 656: train loss: 19.7048397064209\n",
      "Epoch 1 iteration 658: train loss: 24.550228118896484\n",
      "Epoch 1 iteration 660: train loss: 22.589731216430664\n",
      "Epoch 1 iteration 662: train loss: 24.529193878173828\n",
      "Epoch 1 iteration 664: train loss: 21.15821075439453\n",
      "Epoch 1 iteration 666: train loss: 20.328746795654297\n",
      "Epoch 1 iteration 668: train loss: 18.12894630432129\n",
      "Epoch 1 iteration 670: train loss: 20.76067543029785\n",
      "Epoch 1 iteration 672: train loss: 24.701396942138672\n",
      "Epoch 1 iteration 674: train loss: 16.735010147094727\n",
      "Epoch 1 iteration 676: train loss: 10.895064353942871\n",
      "Epoch 1 iteration 678: train loss: 11.787059783935547\n",
      "Epoch 1 iteration 680: train loss: 18.1601505279541\n",
      "Epoch 1 iteration 682: train loss: 12.548762321472168\n",
      "Epoch 1 iteration 684: train loss: 19.736371994018555\n",
      "Epoch 1 iteration 686: train loss: 14.802875518798828\n",
      "Epoch 1 iteration 688: train loss: 16.069978713989258\n",
      "Epoch 1 iteration 690: train loss: 16.337800979614258\n",
      "Epoch 1 iteration 692: train loss: 15.744668960571289\n",
      "Epoch 1 iteration 694: train loss: 21.94896697998047\n",
      "Epoch 1 iteration 696: train loss: 21.09832763671875\n",
      "Epoch 1 iteration 698: train loss: 20.4385986328125\n",
      "Epoch 1 iteration 700: train loss: 21.52618980407715\n",
      "Epoch 1 iteration 702: train loss: 16.72878646850586\n",
      "Epoch 1 iteration 704: train loss: 16.827293395996094\n",
      "Epoch 1 iteration 706: train loss: 18.383182525634766\n",
      "Epoch 1 iteration 708: train loss: 16.51785659790039\n",
      "Epoch 1 iteration 710: train loss: 14.184226989746094\n",
      "Epoch 1 iteration 712: train loss: 17.899852752685547\n",
      "Epoch 1 iteration 714: train loss: 16.18000030517578\n",
      "Epoch 1 iteration 716: train loss: 14.278536796569824\n",
      "Epoch 1 iteration 718: train loss: 16.32474136352539\n",
      "Epoch 1 iteration 720: train loss: 16.14432144165039\n",
      "Epoch 1 iteration 722: train loss: 13.456596374511719\n",
      "Epoch 1 iteration 724: train loss: 16.156225204467773\n",
      "Epoch 1 iteration 726: train loss: 13.130423545837402\n",
      "Epoch 1 iteration 728: train loss: 13.888761520385742\n",
      "Epoch 1 iteration 730: train loss: 19.353879928588867\n",
      "Epoch 1 iteration 732: train loss: 16.22499656677246\n",
      "Epoch 1 iteration 734: train loss: 18.023658752441406\n",
      "Epoch 1 iteration 736: train loss: 15.265009880065918\n",
      "Epoch 1 iteration 738: train loss: 13.698200225830078\n",
      "Epoch 1 iteration 740: train loss: 22.546714782714844\n",
      "Epoch 1 iteration 742: train loss: 24.65750503540039\n",
      "Epoch 1 iteration 744: train loss: 14.52734375\n",
      "Epoch 1 iteration 746: train loss: 19.889972686767578\n",
      "Epoch 1 iteration 748: train loss: 26.246212005615234\n",
      "Epoch 1 iteration 750: train loss: 30.492450714111328\n",
      "Epoch 1 iteration 752: train loss: 11.61648178100586\n",
      "Epoch 1 iteration 754: train loss: 14.094169616699219\n",
      "Epoch 1 iteration 756: train loss: 12.164785385131836\n",
      "Epoch 1 iteration 758: train loss: 17.685428619384766\n",
      "Epoch 1 iteration 760: train loss: 17.528718948364258\n",
      "Epoch 1 iteration 762: train loss: 11.264962196350098\n",
      "Epoch 1 iteration 764: train loss: 15.52933406829834\n",
      "Epoch 1 iteration 766: train loss: 17.01193618774414\n",
      "Epoch 1 iteration 768: train loss: 23.486164093017578\n",
      "Epoch 1 iteration 770: train loss: 15.024782180786133\n",
      "Epoch 1 iteration 772: train loss: 21.38343620300293\n",
      "Epoch 1 iteration 774: train loss: 30.002710342407227\n",
      "Epoch 1 iteration 776: train loss: 23.22931671142578\n",
      "Epoch 1 iteration 778: train loss: 25.506282806396484\n",
      "Epoch 1 iteration 780: train loss: 20.86960220336914\n",
      "Epoch 1 iteration 782: train loss: 19.637636184692383\n",
      "Epoch 1 iteration 784: train loss: 16.01658058166504\n",
      "Epoch 1 iteration 786: train loss: 16.213973999023438\n",
      "Epoch 1 iteration 788: train loss: 25.4702091217041\n",
      "Epoch 1 iteration 790: train loss: 19.964046478271484\n",
      "Epoch 1 iteration 792: train loss: 14.125309944152832\n",
      "Epoch 1 iteration 794: train loss: 10.991485595703125\n",
      "Epoch 1 iteration 796: train loss: 26.267601013183594\n",
      "Epoch 1 iteration 798: train loss: 20.4803524017334\n",
      "Epoch 1 iteration 800: train loss: 14.342504501342773\n",
      "Epoch 1 iteration 802: train loss: 19.973796844482422\n",
      "Epoch 1 iteration 804: train loss: 21.63848876953125\n",
      "Epoch 1 iteration 806: train loss: 25.051952362060547\n",
      "Epoch 1 iteration 808: train loss: 25.216665267944336\n",
      "Epoch 1 iteration 810: train loss: 31.08885955810547\n",
      "Epoch 1 iteration 812: train loss: 25.275436401367188\n",
      "Epoch 1 iteration 814: train loss: 13.776376724243164\n",
      "Epoch 1 iteration 816: train loss: 16.55402946472168\n",
      "Epoch 1 iteration 818: train loss: 22.072961807250977\n",
      "Epoch 1 iteration 820: train loss: 16.35405731201172\n",
      "Epoch 1 iteration 822: train loss: 18.682897567749023\n",
      "Epoch 1 iteration 824: train loss: 21.515316009521484\n",
      "Epoch 1 iteration 826: train loss: 26.498422622680664\n",
      "Epoch 1 iteration 828: train loss: 26.956071853637695\n",
      "Epoch 1 iteration 830: train loss: 20.476608276367188\n",
      "Epoch 1 iteration 832: train loss: 23.087779998779297\n",
      "Epoch 1 iteration 834: train loss: 23.70416259765625\n",
      "Epoch 1 iteration 836: train loss: 24.89411735534668\n",
      "Epoch 1 iteration 838: train loss: 22.078632354736328\n",
      "Epoch 1 iteration 840: train loss: 19.748455047607422\n",
      "Epoch 1 iteration 842: train loss: 21.561403274536133\n",
      "Epoch 1 iteration 844: train loss: 12.825681686401367\n",
      "Epoch 1 iteration 846: train loss: 18.508264541625977\n",
      "Epoch 1 iteration 848: train loss: 21.411983489990234\n",
      "Epoch 1 iteration 850: train loss: 20.07952117919922\n",
      "Epoch 1 iteration 852: train loss: 22.402408599853516\n",
      "Epoch 1 iteration 854: train loss: 22.548917770385742\n",
      "Epoch 1 iteration 856: train loss: 21.322906494140625\n",
      "Epoch 1 iteration 858: train loss: 20.782672882080078\n",
      "Epoch 1 iteration 860: train loss: 14.336156845092773\n",
      "Epoch 1 iteration 862: train loss: 14.758758544921875\n",
      "Epoch 1 iteration 864: train loss: 20.627124786376953\n",
      "Epoch 1 iteration 866: train loss: 15.99509048461914\n",
      "Epoch 1 iteration 868: train loss: 20.847280502319336\n",
      "Epoch 1 iteration 870: train loss: 19.122180938720703\n",
      "Epoch 1 iteration 872: train loss: 17.304691314697266\n",
      "Epoch 1 iteration 874: train loss: 15.293757438659668\n",
      "Epoch 1 iteration 876: train loss: 21.80095672607422\n",
      "Epoch 1 iteration 878: train loss: 17.593801498413086\n",
      "Epoch 1 iteration 880: train loss: 19.265243530273438\n",
      "Epoch 1 iteration 882: train loss: 16.86324119567871\n",
      "Epoch 1 iteration 884: train loss: 21.590051651000977\n",
      "Epoch 1 iteration 886: train loss: 21.335102081298828\n",
      "Epoch 1 iteration 888: train loss: 18.425018310546875\n",
      "Epoch 1 iteration 890: train loss: 20.995723724365234\n",
      "Epoch 1 iteration 892: train loss: 19.568279266357422\n",
      "Epoch 1 iteration 894: train loss: 25.60015106201172\n",
      "Epoch 1 iteration 896: train loss: 23.550384521484375\n",
      "Epoch 1 iteration 898: train loss: 26.125524520874023\n",
      "Epoch 1 iteration 900: train loss: 27.839012145996094\n",
      "Epoch 1 iteration 902: train loss: 25.154964447021484\n",
      "Epoch 1 iteration 904: train loss: 23.645111083984375\n",
      "Epoch 1 iteration 906: train loss: 25.69517707824707\n",
      "Epoch 1 iteration 908: train loss: 27.540119171142578\n",
      "Epoch 1 iteration 910: train loss: 16.6888427734375\n",
      "Epoch 1 iteration 912: train loss: 19.174640655517578\n",
      "Epoch 1 iteration 914: train loss: 16.931055068969727\n",
      "Epoch 1 iteration 916: train loss: 12.711257934570312\n",
      "Epoch 1 iteration 918: train loss: 24.183271408081055\n",
      "Epoch 1 iteration 920: train loss: 16.23018455505371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iteration 922: train loss: 22.219093322753906\n",
      "Epoch 1 iteration 924: train loss: 22.265378952026367\n",
      "Epoch 1 iteration 926: train loss: 19.69928741455078\n",
      "Epoch 1 iteration 928: train loss: 30.89776611328125\n",
      "Epoch 1 iteration 930: train loss: 32.56496047973633\n",
      "Epoch 1 iteration 932: train loss: 29.1979923248291\n",
      "Epoch 1 iteration 934: train loss: 32.97600555419922\n",
      "Epoch 1 iteration 936: train loss: 23.735342025756836\n",
      "Epoch 1 iteration 938: train loss: 19.240589141845703\n",
      "Epoch 1 iteration 940: train loss: 23.656272888183594\n",
      "Epoch 1 iteration 942: train loss: 25.86886978149414\n",
      "Epoch 1 iteration 944: train loss: 30.553871154785156\n",
      "Epoch 1 iteration 946: train loss: 27.950721740722656\n",
      "Epoch 1 iteration 948: train loss: 17.86369514465332\n",
      "Epoch 1 iteration 950: train loss: 32.58040237426758\n",
      "Epoch 1 iteration 952: train loss: 20.128055572509766\n",
      "Epoch 1 iteration 954: train loss: 22.448625564575195\n",
      "Epoch 1 iteration 956: train loss: 21.687255859375\n",
      "Epoch 1 iteration 958: train loss: 20.90441131591797\n",
      "Epoch 1 iteration 960: train loss: 24.872493743896484\n",
      "Epoch 1 iteration 962: train loss: 12.963757514953613\n",
      "Epoch 1 iteration 964: train loss: 18.23282814025879\n",
      "Epoch 1 iteration 966: train loss: 18.791919708251953\n",
      "Epoch 1 iteration 968: train loss: 16.5428409576416\n",
      "Epoch 1 iteration 970: train loss: 24.48748207092285\n",
      "Epoch 1 iteration 972: train loss: 21.339427947998047\n",
      "Epoch 1 iteration 974: train loss: 17.922176361083984\n",
      "Epoch 1 iteration 976: train loss: 17.82184410095215\n",
      "Epoch 1 iteration 978: train loss: 18.268505096435547\n",
      "Epoch 1 iteration 980: train loss: 20.914308547973633\n",
      "Epoch 1 iteration 982: train loss: 20.63041114807129\n",
      "Epoch 1 iteration 984: train loss: 22.853586196899414\n",
      "Epoch 1 iteration 986: train loss: 18.307861328125\n",
      "Epoch 1 iteration 988: train loss: 17.703184127807617\n",
      "Epoch 1 iteration 990: train loss: 17.515274047851562\n",
      "Epoch 1 iteration 992: train loss: 23.112850189208984\n",
      "Epoch 1 iteration 994: train loss: 17.354860305786133\n",
      "Epoch 1 iteration 996: train loss: 20.813169479370117\n",
      "Epoch 1 iteration 998: train loss: 20.082956314086914\n",
      "Epoch 1 iteration 1000: train loss: 20.552114486694336\n",
      "Epoch 1 iteration 1002: train loss: 20.425451278686523\n",
      "Epoch 1 iteration 1004: train loss: 16.93456268310547"
     ]
    }
   ],
   "source": [
    "train_loss_, val_loss_ = train(model, optimizer, \n",
    "      criterion, n_epochs, scheduler, \n",
    "      debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
